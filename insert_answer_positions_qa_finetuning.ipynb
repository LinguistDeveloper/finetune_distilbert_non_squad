{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c12e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60e8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b4b4c5bb-73b9-44e9-a7f3-6549af5e5242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "# Load a tokenizer (e.g., BERT-based)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e58f8ee-d617-4ba8-836c-72b55ed80403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4325cfed-0822-4e6c-8e55-d02e3c96addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook.__version__== '6.5.2':\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    # Create file upload widget\n",
    "    uploader = widgets.FileUpload(accept='', multiple=False)  # Accept all files, allow single upload\n",
    "\n",
    "    # Display widget\n",
    "    display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "283041cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work with Jupyter Notebook version 7.0.8\n",
    "\n",
    "if notebook.__version__== '6.5.2':\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import io\n",
    "\n",
    "    if uploader.value:\n",
    "        uploaded_file = next(iter(uploader.value.values()))  # Get the first uploaded file\n",
    "    #    t_name_without_ext = Path(uploaded_file).stem\n",
    "\n",
    "        file_content = uploaded_file['content']  # Extract file content as bytes\n",
    "    \n",
    "        # Convert bytes to a file-like object\n",
    "        file_buffer = io.BytesIO(file_content)\n",
    "    \n",
    "        # Read CSV from the buffer\n",
    "        tdf = pd.read_csv(file_buffer, sep=\"\\t\")\n",
    "    #    t_name_without_ext = Path(tdf).stem\n",
    "\n",
    "    #    print(tdf.head())  # Show the first few rows\n",
    "        file_name = uploaded_file['metadata']['name']  # Extract filename\n",
    "    \n",
    "        print(f\"Uploaded File Name: {file_name}\")\n",
    "    #    file_name_without_ext = Path(tdf).stem\n",
    "\n",
    "        file_name_no_ext = Path(file_name).stem  # Remove extension\n",
    "\n",
    "        print(f\"File Name (Without Extension): {file_name_no_ext}\")\n",
    "\n",
    "        print(type(tdf))\n",
    "        \n",
    "    # Convert to JSON\n",
    "    json_training_data = tdf.to_json(orient=\"records\")\n",
    "    #print(json_data)\n",
    "\n",
    "\n",
    "    # Save the JSON file\n",
    "    #with open(\"squad_sample_train.json\", \"w\") as json_file:\n",
    "    with open(f\"{file_name_no_ext}.json\", \"w\") as json_training_file:\n",
    "        json_training_file.write(json_training_data)\n",
    "\n",
    "    print(f\"Conversion complete: {file_name_no_ext}.json created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f918d31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Convert to JSON\\njson_training_data = tdf.to_json(orient=\"records\")\\n#print(json_data)\\n\\n\\n# Save the JSON file\\n#with open(\"squad_sample_train.json\", \"w\") as json_file:\\nwith open(f\"{t_name_without_ext}.json\", \"w\") as json_training_file:\\n    json_training_file.write(json_training_data)\\n\\nprint(f\"Conversion complete: {file_name_no_ext}.json created\")'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Convert to JSON\n",
    "json_training_data = tdf.to_json(orient=\"records\")\n",
    "#print(json_data)\n",
    "\n",
    "\n",
    "# Save the JSON file\n",
    "#with open(\"squad_sample_train.json\", \"w\") as json_file:\n",
    "with open(f\"{t_name_without_ext}.json\", \"w\") as json_training_file:\n",
    "    json_training_file.write(json_training_data)\n",
    "\n",
    "print(f\"Conversion complete: {file_name_no_ext}.json created\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "791d28d4-8eb7-4a49-bd76-6eaacc045620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a training data filename. squad_sample_train.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Conversion complete: squad_sample_train.json created\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the TSV file\n",
    "#df = pd.read_csv(\"squad_sample_train.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "tf = input(\"Please enter a training data filename.\")\n",
    "tdf = pd.read_csv(tf, sep=\"\\t\")\n",
    "print(type(tdf))\n",
    "#print(df)\n",
    "t_name_without_ext = Path(tf).stem\n",
    "\n",
    "# Convert to JSON\n",
    "json_training_data = tdf.to_json(orient=\"records\", indent=4)\n",
    "#print(json_data)\n",
    "\n",
    "# Save the JSON file\n",
    "#with open(\"squad_sample_train.json\", \"w\") as json_file:\n",
    "with open(f\"{t_name_without_ext}.json\", \"w\") as json_training_file:\n",
    "    json_training_file.write(json_training_data)\n",
    "\n",
    "print(f\"Conversion complete: {t_name_without_ext}.json created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "564b8f3e-10d7-4479-8609-082d7839062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a validation data filename. squad_sample_validation.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete: squad_sample_validation.json created\n"
     ]
    }
   ],
   "source": [
    "# Load the TSV file\n",
    "#df = pd.read_csv(\"squad_sample_validation.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "vf = input(\"Please enter a validation data filename.\")\n",
    "vdf = pd.read_csv(vf, sep=\"\\t\")\n",
    "#print(df)\n",
    "v_name_without_ext = Path(vf).stem\n",
    "\n",
    "# Convert to JSON\n",
    "json_validation_data = vdf.to_json(orient=\"records\", indent=4)\n",
    "#print(json_data)\n",
    "\n",
    "# Save the JSON file\n",
    "#with open(\"squad_sample_train.json\", \"w\") as json_file:\n",
    "with open(f\"{v_name_without_ext}.json\", \"w\") as json_validation_file:\n",
    "    json_validation_file.write(json_validation_data)\n",
    "\n",
    "print(f\"Conversion complete: {v_name_without_ext}.json created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a6889c4-24f9-4b33-987b-1f8b98b38619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(json_training_data, str):  \n",
    "    json_training_data = json.loads(json_training_data)  \n",
    "\n",
    "\n",
    "#def insert_context_placeholders(training_data, id, title=title, context, question, answer):\n",
    "def insert_context_placeholders_tr(training_data, id=None, title=None,  contexts=None, question=None, answers=None):\n",
    "\n",
    "    id = []\n",
    "    title = []\n",
    "    contxt = []\n",
    "    question = []\n",
    "    answ = []\n",
    "\n",
    "    for entry in training_data:\n",
    "#        print(entry)\n",
    "#        print(entry[\"title\"])\n",
    "\n",
    "        v = entry[\"id\"]\n",
    "#        print(v)\n",
    "        id.append(v)\n",
    "        v = entry[\"title\"]\n",
    "#        print(v)\n",
    "        title.append(v)\n",
    "        v = entry[\"question\"]\n",
    "#        print(v)\n",
    "        question.append(v)\n",
    "        context = entry[\"context\"]\n",
    "        answers = entry[\"answers\"]\n",
    "        # Find the position of the answer\n",
    "        answer_start = context.find(answers)\n",
    "        answer_end = answer_start + len(answers)\n",
    "\n",
    "#        print(answer_start)\n",
    "#        print(answer_end)\n",
    "\n",
    "        # Tokenize while preserving position info\n",
    "        tokens = tokenizer(context, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "\n",
    "#        print(tokens)\n",
    "\n",
    "        # Find token-level positions for the answer\n",
    "        start_token, end_token = None, None\n",
    "        for idx, (start, end) in enumerate(tokens[\"offset_mapping\"].squeeze().tolist()):\n",
    "            if start == answer_start:\n",
    "                start_token = idx\n",
    "            if end == answer_end:\n",
    "                end_token = idx\n",
    "                break  # Stop once we find the end token\n",
    "        answers = {\n",
    "                \"text\": [answers],\n",
    "                \"answer_start\": [answer_start]\n",
    "            }\n",
    "#   abs print(answers)\n",
    "        answ.append(answers)\n",
    "        contxt.append(context)\n",
    "    training_dataset = Dataset.from_dict({\n",
    "        \"id\": id,\n",
    "        \"title\": title,\n",
    "        \"context\": contxt,\n",
    "        \"question\": question,\n",
    "        \"answers\": answ\n",
    "    })\n",
    "\n",
    "#    print(\"Training dataset: \",training_dataset)\n",
    "    \n",
    "    return training_dataset\n",
    "    \n",
    "insert_context_placeholders_tr(json_training_data, [], [], [], [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1add3bf1-ee04-40b5-bee4-64dda704ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = insert_context_placeholders_tr(json_training_data, [], [], [], [], [])\n",
    "#print(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc938466-e127-4b28-afcd-522d5a94f17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(json_validation_data, str):  \n",
    "    json_validation_data = json.loads(json_validation_data)  \n",
    "\n",
    "\n",
    "def insert_context_placeholders_val(validation_data, id=None, title=None,  contexts=None, question=None, answers=None):\n",
    "\n",
    "    id = []\n",
    "    title = []\n",
    "    contxt = []\n",
    "    question = []\n",
    "    answ = []\n",
    "\n",
    "    for entry in validation_data:\n",
    "#        print(entry)\n",
    "#        print(entry[\"title\"])\n",
    "\n",
    "        v = entry[\"id\"]\n",
    "#        print(v)\n",
    "        id.append(v)\n",
    "        v = entry[\"title\"]\n",
    "#        print(v)\n",
    "        title.append(v)\n",
    "        v = entry[\"question\"]\n",
    "#        print(v)\n",
    "        question.append(v)\n",
    "        context = entry[\"context\"]\n",
    "        answers = entry[\"answers\"]\n",
    "        # Find the position of the answer\n",
    "        answer_start = context.find(answers)\n",
    "        answer_end = answer_start + len(answers)\n",
    "\n",
    "#        print(answer_start)\n",
    "#        print(answer_end)\n",
    "\n",
    "        # Tokenize while preserving position info\n",
    "        tokens = tokenizer(context, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "\n",
    "#        print(tokens)\n",
    "\n",
    "        # Find token-level positions for the answer\n",
    "        start_token, end_token = None, None\n",
    "        for idx, (start, end) in enumerate(tokens[\"offset_mapping\"].squeeze().tolist()):\n",
    "            if start == answer_start:\n",
    "                start_token = idx\n",
    "            if end == answer_end:\n",
    "                end_token = idx\n",
    "                break  # Stop once we find the end token\n",
    "        answers = {\n",
    "                \"text\": [answers],\n",
    "                \"answer_start\": [answer_start]\n",
    "            }\n",
    "#   abs print(answers)\n",
    "        answ.append(answers)\n",
    "        contxt.append(context)\n",
    "    validation_dataset = Dataset.from_dict({\n",
    "        \"id\": id,\n",
    "        \"title\": title,\n",
    "        \"context\": contxt,\n",
    "        \"question\": question,\n",
    "        \"answers\": answ\n",
    "    })\n",
    "\n",
    "#    print(\"Validation dataset: \",validation_dataset)\n",
    "\n",
    "    return validation_dataset\n",
    "    \n",
    "insert_context_placeholders_val(json_validation_data, [], [], [], [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b9e82f51-79cd-492c-8f91-e0aadcd53978",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = insert_context_placeholders_val(json_validation_data, [], [], [], [], [])\n",
    "#print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47de79bc-1c70-4c7e-9d54-dc2ece2fa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_sample = DatasetDict({\n",
    "    \"train\": training_dataset,  # Add more splits (e.g., validation, test) as needed\n",
    "    \"validation\": validation_dataset  # Add more splits (e.g., validation, test) as needed\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c6539824-b3ca-4a12-b21f-bc934213716c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec1aa904-6737-4d00-bc64-e4f3df682f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    print(examples[\"question\"])\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"], examples[\"context\"],\n",
    "        max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Extract start and end positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, answer in enumerate(examples[\"answers\"]):\n",
    "        if len(answer[\"answer_start\"]) > 0:\n",
    "            start = answer[\"answer_start\"][0]\n",
    "            end = start + len(answer[\"text\"][0])\n",
    "        else:\n",
    "            start = 0\n",
    "            end = 0\n",
    "        start = 0\n",
    "        end = 0\n",
    "        start_positions.append(start)\n",
    "        end_positions.append(end)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    inputs[\"input_ids\"] = torch.tensor(inputs[\"input_ids\"])\n",
    "    inputs[\"attention_mask\"] = torch.tensor(inputs[\"attention_mask\"])\n",
    "    inputs[\"start_positions\"] = torch.tensor(start_positions)\n",
    "    inputs[\"end_positions\"] = torch.tensor(end_positions)\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e7254dfb-b51f-40ed-afb8-33320ee59b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16348bb2170447418b880ff1a9153436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is in front of the Notre Dame Main Building?', 'What is the Grotto at Notre Dame?', 'What is the primary seminary of the Congregation of the Holy Cross?', 'Which company is Danjaq, LLC associated with?', 'Which film studio won the full copyright film rights to Spectre?']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22d73994f9f4b4a974d2e68d5de84de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whose name was added to the name for every town or city with historical signiciance from the World War II period?', \"What was the name of Tito's personal doctor?\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized_squad = squad_sample.map(preprocess_function, batched=True, remove_columns=squad_sample[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "99a3babc-6e0f-405b-a253-2f070b5f98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format to PyTorch tensors\n",
    "tokenized_squad.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "977f2da7-bf68-4118-99cf-2c9d89fe42b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7404c854-72ab-4499-97d9-541c3baafeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_squad[\"train\"], batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(tokenized_squad[\"validation\"], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74fd684d-ff06-429b-b861-e2cb398e8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1ac50c2a-8248-49b0-afb0-113329bd4d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.341365814208984\n",
      "Epoch 1, Loss: 6.085456371307373\n",
      "Epoch 2, Loss: 5.885241508483887\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "66e4cdc7-59b2-4421-844c-577f3f0c06a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model...\n",
      "Model saved!\n",
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForQuestionAnswering'>\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "print(\"Saving the model...\")\n",
    "model.save_pretrained(\"./distilbert-squad7\")\n",
    "tokenizer.save_pretrained(\"./distilbert-squad7\")\n",
    "print(\"Model saved!\")\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3b7ba5e3-30eb-4310-bb80-94437c348ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: [CLS] what is the capital of france?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "\n",
    "# Load fine-tuned model (Replace with your actual model path if needed)\n",
    "#model_name = \"distilbert-base-uncased-distilled-squad\"  # Or your fine-tuned checkpoint\n",
    "#model_name = model(\"./distilbert-squad6\")\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained(\"./distilbert-squad6\")\n",
    "#model_name = model\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "#model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"./distilbert-squad7\")\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"./distilbert-squad7\")\n",
    "\n",
    "# Define your Question and Context\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France's capital city is Paris. It is known for the Eiffel Tower.\"\n",
    "#question = \"What is the capital of Russia?\"\n",
    "#context = \"Russia's capital city is not St Petersburg but Moscow. It is known for the Kremlin.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
    "#print(inputs)\n",
    "\n",
    "# Run the model to get start & end logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract predicted answer positions\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Convert logits to token positions\n",
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1  # +1 because slicing is exclusive\n",
    "\n",
    "# Convert token indexes back to text\n",
    "predicted_answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
    ")\n",
    "\n",
    "print(\"Predicted Answer:\", predicted_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7e22047f-61a8-402a-997c-06eb5ae244b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "\n",
    "# Remove token_type_ids if present\n",
    "#if \"token_type_ids\" in inputs:\n",
    "#    del inputs[\"token_type_ids\"]\n",
    "\n",
    "# Pass the modified inputs to the model\n",
    "#result = qa_pipeline({\"question\": question, \"context\": context})\n",
    "\n",
    "# Create a QA pipeline for inference\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aae6e323-0cf3-4771-9078-82aa90f70792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load evaluation metrics from Hugging Face\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    return metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0ea05cda-40d4-4127-bda7-6fb83b025fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"squad\", \"rc\", split=\"validation[:1]\")  # Take 100 samples for quick testing\n",
    "dataset = load_dataset(\"squad\", split=\"validation[:1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a3457183-9e6e-48db-9a7c-cafbf502ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.0, 'f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Format dataset for evaluation\n",
    "predictions, references = [], []\n",
    "\n",
    "for example in dataset:\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "    ground_truths = example[\"answers\"][\"text\"]  # Extract answers\n",
    "    answer_starts = example[\"answers\"][\"answer_start\"]  # Extract answer positions\n",
    "\n",
    "    # Get model prediction\n",
    "    result = qa_pipeline({\"question\": question, \"context\": context})\n",
    "#    result = qa_pipeline(question=question, context=context)  # Call the pipeline properly\n",
    "    pred_answer = result[\"answer\"]  # Extract only the predicted text\n",
    "\n",
    "    # Store references in SQuAD format\n",
    "    references.append({\n",
    "        \"id\": example[\"id\"],\n",
    "        \"answers\": [{\"text\": ans, \"answer_start\": start} for ans, start in zip(ground_truths, answer_starts)]\n",
    "    })\n",
    "\n",
    "    # Store predictions\n",
    "    predictions.append({\"id\": example[\"id\"], \"prediction_text\": pred_answer})\n",
    "\n",
    "# Compute EM and F1 scores\n",
    "scores = compute_metrics(predictions, references)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886c173-320b-499f-8255-6e519964e1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
